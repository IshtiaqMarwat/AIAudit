# -*- coding: utf-8 -*-
"""audit_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aPMNEUozOX1LJ1bVfEltXY8is0SsNXsH

# ğŸ“ Streamlit: Audit Assistant App
"""

# ğŸ“ Streamlit: Audit Assistant App

import streamlit as st
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

# Optional: If running locally or deployed, adjust this path
vector_db_path = "/content/drive/MyDrive/audit_vector_db"  # Full path to saved FAISS index

# ğŸ” OpenAI API Key
openai_api_key = st.secrets.get("OPENAI_API_KEY") or st.text_input("Enter your OpenAI API Key", type="password")

# Load the FAISS vector DB from Google Drive
@st.cache_resource
def load_vector_db():
    embeddings = HuggingFaceEmbeddings()
    return FAISS.load_local(vector_db_path, embeddings)

# Load and setup
vector_db = load_vector_db()
llm = ChatOpenAI(model="gpt-4o", temperature=0, api_key=openai_api_key)
retriever = vector_db.as_retriever(search_type="similarity", search_kwargs={"k": 4})
qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)

# Streamlit UI
st.title("ğŸ•µï¸ Audit Assistant")
st.markdown("Ask any question related to your audit documents that were preloaded into the assistant.")

question = st.text_input("ğŸ” Your question:")

if question:
    with st.spinner("ğŸ” Searching for answers..."):
        result = qa_chain.invoke(question)
        st.text_area("ğŸ§  Audit Assistant's Answer", value=result["result"], height=200)

